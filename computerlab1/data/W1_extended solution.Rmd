---
title: "Computer workshop 1 (NBS8186)"
author: "Edu Gonzalo Almorox"
date: 
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document contains commented solutions to the questions of Workshop 1 for _NBS8186 Introductory Economics_. Analysis is based on `nba.csv` file.

## Question A

_Download the data set from Blackboard and save it on your h: drive. Then open the data set in R and make it the default data set._


Prior to load the data you must set the working directory in your computer. The working directory is the place in your computer where you allocate the information that you are going to use in your data analysis. `setwd()` is the function to tell R the working directory in your computer^[Note: The way to introduce the path may differ in case you are using Windows or Mac. For Windows it looks like `"C:/Users/User Name/Documents/FOLDER"` whereas for Mac it is similar to `"/Users/User Name/Documents/FOLDER"`]. Sometimes you may be using several working directories. In order to know the current working directory you are working on you may use `getwd()` 


```{r include = TRUE, echo = TRUE}

setwd("/Users/Personas/My Cloud/PhD _october_2016/teaching/NBS8186/data")
getwd() # what working directory?
```





Once the working directory is established, it is time to load the data. The most common way to input a dataset in R consists of using the base^[There are other packages such as `foreign` or `rio` that can also be used for loading data.] function `read.csv()`.

```{r include = TRUE, echo =TRUE}

nba = read.csv("nba.csv", sep = ",", header = TRUE)

```

The data are in your computer, now may carry out some exploratory analysis of your data. For example you may want to have a look at the first and last rows. This can be done using `head()` and `tail()`.

```{r include = TRUE, echo =TRUE}

head(nba, 5) # gives the first five lines 

tail(nba, 5) # gives the last five lines
  
```

Also, it is possible to see the structure of your data frame using `str()`


```{r include = TRUE, echo =TRUE}

str(nba) 
  
```

## Question B

_Have a look at the summary statistics of the data set._

`summary()` is used to get a summary statistics of the variables in your data frame.

```{r include = TRUE, echo =TRUE}

summary(nba) 

```

_What is the average age of the players?_


According to the results displayed by `summary()` we can see that the average age is 27.4 years. An alternative way to obtain the average age would be by calling directly the variable `age` using the operator `$`. 

```{r include = TRUE, echo =TRUE}

mean(nba$age) 

```

_How many play forwards?_

`forward` is a categorical variable. In these variables, numbers indicate qualitative characteristics that cannot be measured. This type of variables are called `factors()` in R. A simple way to summarise the number of categories in a factor is by using `table()`^[There are alternative and more efficient ways to carry out this task. `data.table` and `dplyr` are the most suited packages when there are bigger samples.]

```{r include = TRUE, echo =TRUE}

table(nba$forward)

```

We can see that 109 players play forwards against 158 that play in other positions i.e. center and guard.

## Question C 

_Plot a histogram of points-per-game._

Histograms give a visual idea of the frequency distribution of a variable. A way for plotting a simple histogram would be by using `hist()` and adding `points` variable. 

```{r include = TRUE, echo =TRUE}

# Histogram 

    # xlab = rename the axis X
    # main = title of the plot      
    
hist(nba$points, xlab= "points", main = "Histogram of points")
  

```


## Question D 

_Produce a scatterplot of points-per-game versus years in league._

Scatterplots represent the association between two variables. A way of doing it is by using function `plot()`. Until now we have been using an object (e.g variables) that is "residing" in another object (e.g. a data frame). The easy (and natural) way to refer to them is by indexing with `$`. However, when there are more objects involved (e.g. several variables), typing `$` systematically can be confusing (specially with long names) and produce errors. In order to avoid this, it is possible to use `with()` to _attach_ the data frame and use the variables independently^[R has a `attach()` function that can be used to make objects within dataframes accessible in R without calling to the data frame. Yet the use of this function is not recommended. See the [Google R Style Guide](https://google.github.io/styleguide/Rguide.xml#attach) for details.].

```{r include = TRUE, echo =TRUE}

# Scatterplot 

    # xlab = rename the axis X
    # ylab = rename the axis Y
    # main = title of the plot      
    
with(nba, plot(points, exper, xlab= "points", ylab = "experience", 
               main = "Scatterplot of points vs experience"))


```

## Question E

_Run a regression of points-per-game on years in league, age, years played in college and position dummies._

```{r include = TRUE, echo =TRUE, message = FALSE}

  library(stats)
  library(stargazer)

 model1 = lm(points ~ exper + age + coll + forward + center, data = nba)
 summary(model1)
```

`lm` estimates a linear model using ordinary least squares (OLS) and returns `model1`, a fitted-model object^[The components of this object can be retrieved by using the function `names()`]. The variable before "` ~ `" indicates the dependent variable whereas the variables in the right side are considered the set of explanatory regressors. `summary()` allow to visualise the output of the regression.

```{r, message = FALSE, echo = TRUE, results = 'asis'}
stargazer(model1,  type = "latex",
          title = "Points per game", header = FALSE)
```

`stargazer()` produces a table (Table 1) with the results from the regression corresponding to fitted model object `model1`. 

We can see that the experience (`exper`) has a statistically significant influence in the perfomance. Concretely an additional year of experience implies 1.4 additional points per game. Age (`age`) and years playing at college (`coll`) play a negative role. Whereas the fact of being a year older deteriorates the performance by 1.13 points per game, having played in college before seems to decrease the performance in about 1.2 points per game. Both negative effects are statistically significant. In general, all variables with the exception to `forward` are statistically significant at the 5% level of significance. 

The $R^2$ indicates the level of variance in the data that is explained by the model. In this case is about the 13%. Likewise, the F-statistic indicates the results of an F test of the hypothesis that all regressors are jointly significant. In this case the intercept term is excluded. 

## Question F

If players can be drafted in early years (e.g. college or high school in some cases) then the negative effect on the performance may be capturing the time when players are not essentially playing in the NBA and therefore not scoring points either. 

## Question G

_Look at the correlation matrix_

`rcorr()` is the function for creating a correlation matrix and obtaining the levels of significance. `pander()` creates a table with the results of the correlation matrix (Table 2).

```{r include = TRUE, echo =TRUE, message = FALSE}

library(Hmisc)
library(dplyr)
library(devtools)
library(pander)

# select variables from the model 
vars_mod = nba %>% select(exper, age, coll, forward, center)

# correlation matrix 
cor_mat <- rcorr(as.matrix(vars_mod), type = "pearson")
emphasize.strong.cells(which(cor_mat[[3]] < 0.001, arr.ind = TRUE))
pander(cor_mat[[1]], caption = "Correlation matrix")

```

We can also extract the p-values associated with the  significance levels of the correlations. Hence, the p-values determine whether the correlations are significant. Analogously, the p-values can be represented in a table with `pander()` (Table 3).

```{r include = TRUE, echo =TRUE, message = FALSE}
pander(cor_mat[[3]], caption = "Correlation matrix (p-values)")

```

_Do you need to worry about multicollinearity?_

Normally, a strong correlation is considered when the magnitude of the Pearson correlation coefficient ($r$) is > 0.5. According to the results from the correlation matrix, `exper` and `age` show high positive correlation (0.94). Moreover, this correlation is significant as we can see in Table 3. 


## Question H

_Now consider an extension of the basic model. Generate a new variable which is experience squared and include it in the regression._


With `mutate` from `dplyr` it is possible to create new variables. 

```{r include = TRUE, echo =TRUE, message = FALSE}

library(dplyr)
library(tibble)

# create a new variable 

nba = nba %>% mutate(expersq = exper^2)
head(nba)

```

_Holding age, coll, center and forward fixed, at what value of experience does the next year of experience reduce points-per-game?_

We run first the model including the new variable `expersq`

```{r include = TRUE, echo =TRUE, message = FALSE}

  library(stats)
  library(stargazer)
  
 model2 = lm(points ~ exper+expersq+age+coll+center+forward, data = nba)
 
```

```{r, message = FALSE, echo = TRUE, results = 'asis'}
stargazer(model2, type = "latex", 
          title = "Points per game (model 2)", header = FALSE)

```

Table 4 reflects the results of this new model (`model2`). The experience is not a positive factor for the performance at 0.072. This is a plausible result since players apart from being more experienced also get older. 

## Question I

_Now you want to explain the log(wage)_

Similarly to former exercises, we create `logwage` by using `mutate()`.

```{r include = TRUE, echo =TRUE, message = FALSE}

library(dplyr)

# create a new variable 

nba = nba %>% mutate(logwage = log(wage))
head(nba)
```

For estimating the new model (`model3`) we apply the following code 

```{r include = TRUE, echo =TRUE, message = FALSE}

model3 <- lm(logwage~points+exper+expersq+age+coll, data = nba)
```

```{r, message = FALSE, echo = TRUE, results = 'asis'}
stargazer(model3, type = "latex",
          title =  "Model 3", header = FALSE)

```

_How do you interpret the results?_

The results are contained in Table 5. The interpretation of the variables differs both regressors and dependent variables are transformed. A log transformation in depedent variable in this case will interpreted as a percent change. Hence, in this case the wage seems to be more influenced by the experience than the performance. Particularly, whereas the points obtained would suppose an increase of 7% in the wage, having an additional year of experience would suppose an increase of the 22.3%. 

## Question J 

_Test whether age and coll are jointly significant in the regression from (i). What does this imply about whether age and education have a separate effect on wage, once productivity and senority are controlled for? (Hint: You need to estimate both the unrestricted and the restricted model. Then you can use the anova() command to get the sums of squared residuals for the two models.)_ 



```{r include = TRUE, echo =TRUE, message = FALSE}

library(pander)
mod_unrest <- lm(logwage~points+exper+expersq+age+coll, data = nba)
mod_rest <- lm(logwage~points+exper+expersq, data = nba)

anova(mod_rest, mod_unrest)

pander(anova(mod_rest, mod_unrest), caption = "Comparison of models")
```



`anova()` can be used for model comparison. Table 6 shows the results derived from the `ANOVA` test. The models we are comparing are nested - i.e. both models share a set of regressors and have the same outcome but one of them (e.g. the unrestricted model) has additional regressors. The results of Table 4 reveal that the p value is 0.292. This indicates that the joint variance of two variables such as `age`  and `coll` is not meaningful to the model so it is not possible to reject the null hypothesis that both coefficients are 0. Therefore, the changes in the wage as a result of `age` and `coll` are 0. 

